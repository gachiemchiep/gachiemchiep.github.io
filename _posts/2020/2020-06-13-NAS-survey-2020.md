---
title: "A quick look at NAS (Neural Architecture Search)"
excerpt: "A quick look at NAS (Neural Architecture Search)."
categories: 
  - Machine Learning
tags: 
  - note 
comments: true
toc: true
support: true
published: true
order: 9
author: vugia.truong
---

2020 NAS surveyr [A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions](https://arxiv.org/abs/2006.02903)

The current research results are as follows:

![img_001](/assets/images/2020/20200613_001.png)

![img_002](/assets/images/2020/20200613_002.png)

![img_003](/assets/images/2020/20200613_003.png)

![img_004](/assets/images/2020/20200613_004.png)

There're five type of methods used in architecture search. 

1. reinforcement learning (RL), 
2. evolutionary algorithm (EA), 
3. gradient optimization (GO), 
4. random search (RS) 
5. sequential model-based optimization (SMBO).

I don't understand much about NAS. But from above result, Neural Architectures created by machine have bigger number of parameters and lower precision. So using human-designed neural network architecture is still better for now. Hope there will be some big improvement in NAS in the near future. 

There're some other useful link

1. [Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)
2. [PC-DARTS](https://github.com/yuhuixu1993/PC-DARTS)
3. [DARTS](https://github.com/quark0/darts)

End